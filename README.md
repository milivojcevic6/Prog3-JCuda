üëâ [Scroll up for English version](#jcuda-examples)


# JCuda Primeri

Ta repozitorij vsebuje **osnovne primere [JCuda](https://javagl.de/jcuda.org/)**, ustvarjene med sledenjem vadnici za razumevanje, kako lahko GPU-je s podporo za CUDA uporabljamo iz Jave. Primeri postopoma napredujejo od preprostega preverjanja delovanja do dejanskih izraƒçunov na GPU-ju in primerjav zmogljivosti s CPU-jem.

Cilji tega repozitorija so:

* Preveriti, ali je **JCuda pravilno name≈°ƒçena in delujoƒça**
* Uporabljati **CUDA knji≈ænice (JCurand)** iz Jave
* Write and execute a **custom CUDA kernel** from Java
* Napisati in zagnati **lastno CUDA jedro (kernel)** iz Jave
* Primerjati zmogljivost **CPU-ja in GPU-ja**, vkljuƒçno z vplivom prenosa podatkov

---

## Predpogoji

* NVIDIA GPU s podporo za CUDA
* Name≈°ƒçen [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads) (`nvcc` mora biti na voljo v terminalu)
* Java (priporoƒçeno JDK 8 ali novej≈°i)
* Pravilno [nastavljene](https://javagl.de/jcuda.org/downloads/downloads.html) JCuda knji≈ænice

---

## 1. Preverjanje nastavitve JCuda ‚Äì `Welcome.java`

To je **minimalni test**, s katerim preverimo, da:

* je JCuda pravilno povezana
* je CUDA runtime dostopen
* dodeljevanje pomnilnika na GPU-ju deluje

### Kaj program naredi

* Dodeli 4 bajte pomnilnika na GPU-ju z `cudaMalloc`
* Izpi≈°e naslov kazalca (pointer)
* Sprosti dodeljeni pomnilnik

ƒåe se program izvede brez napak, **JCuda deluje pravilno**.

---

## 2. Generiranje nakljuƒçnega polja ‚Äì `RandomArray.java`

Ta primer prikazuje uporabo knji≈ænice **[JCurand](https://javagl.de/jcuda.org/jcuda/jcurand/JCurand.html)** za generiranje nakljuƒçnih ≈°tevil na GPU-ju ter primerjavo zmogljivosti s CPU implementacijo.

### Kaj program naredi

* Ustvari polje `n` nakljuƒçnih realnih ≈°tevil (float)
* **CPU razliƒçica**: zaporedno generiranje nakljuƒçnih ≈°tevil
* **GPU razliƒçica**: generiranje nakljuƒçnih ≈°tevil z uporabo `JCurand`
* Izmeri in primerja ƒças izvajanja obeh pristopov

### Kljuƒçni koncepti

* Uporaba **CUDA knji≈ænic iz Jave**
* Generiranje nakljuƒçnih ≈°tevil na GPU-ju
* Primerjava zmogljivosti med:

  * CPU (zaporedno)
  * GPU (vzporedno prek JCuda)

Ta primer poka≈æe, da GPU ni vedno najbolj≈°a izbira: pri majhnih ali trivialnih izraƒçunih lahko re≈æijski stro≈°ki uporabe GPU-ja zmanj≈°ajo ali izniƒçijo prednosti v zmogljivosti.

---

## 3. Mno≈æenje matrik z lastnim jedrom

Vkljuƒçene datoteke:

* `MatrixMultiplication.java`
* `kernel.cu`

To je **najnaprednej≈°i primer** v repozitoriju, ki prikazuje, kako:

* napisati lastno CUDA jedro
* ga prevesti v PTX obliko
* ga zagnati iz Jave z uporabo JCuda
* primerjati zmogljivost CPU-ja in GPU-ja

### Kaj program naredi

* Izvede mno≈æenje matrik: **C = A √ó B**
* Implementira:

  * CPU razliƒçico (zaporedno)
  * GPU razliƒçico (CUDA jedro)
* Izmeri ƒças izvajanja v razliƒçnih pogojih

### Opazovanja glede zmogljivosti

* Pri velikih matrikah (npr. `n = m = k = 1024`):

  * **GPU je pribli≈æno 10√ó hitrej≈°i**, ƒçe upo≈°tevamo tudi dodeljevanje in prenos pomnilnika
  * **GPU je pribli≈æno 1000√ó hitrej≈°i**, ƒçe primerjamo samo:

    * izvajanje jedra + sinhronizacijo
    * ƒças CPU izraƒçuna

To jasno poka≈æe, da:

* so CUDA jedra izjemno hitra
* ma prenos podatkov in dodeljevanje pomnilnika velik vpliv
* so dobitki najveƒçji pri **velikih delovnih obremenitvah**

---

## Prevajanjem CUDA jedra

CUDA jedro je zapisano v datoteki `kernel.cu` in ga je pred zagonom Java kode treba prevesti v **PTX** obliko.

### Ukaz za prevajanje

```bash
nvcc -ptx kernel.cu -o kernel.ptx
```

Ustvarjena datoteka `kernel.ptx` se nato nalo≈æi v razredu `MatrixMultiplication.java` med izvajanjem.

---

## Opombe

* Zmogljivost je moƒçno odvisna od modela GPU-ja in konfiguracije sistema
* Pri majhnih vhodnih podatkih se rezultati razlikujejo zaradi re≈æijskih stro≈°kov
* Primeri so namenjeni **izobra≈æevalnim namenom**


# JCuda Examples

This repository contains **basic [JCuda](https://javagl.de/jcuda.org/) examples** created while following a tutorial to understand how **CUDA-enabled GPUs can be used from Java**. The examples progress from a simple sanity check to real GPU computation and performance comparison with CPU implementations.

The goal of this repo is to:

* Verify that **JCuda is correctly installed and runnable**
* Use **CUDA libraries (JCurand)** from Java
* Write and execute a **custom CUDA kernel** from Java
* Compare **CPU vs GPU performance**, including the effect of memory transfers

---

## Prerequisites

* NVIDIA GPU with CUDA support
* [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads) installed (`nvcc` available in terminal)
* Java (JDK 8+ recommended)
* JCuda libraries properly set up

---

## 1. JCuda Setup Test ‚Äì `Welcome.java`

This is a **minimal sanity check** to verify that:

* JCuda is correctly linked
* CUDA runtime is accessible
* GPU memory allocation works

### What it does

* Allocates 4 bytes on the GPU using `cudaMalloc`
* Prints the pointer address
* Frees the allocated memory

If this program runs without errors, **JCuda is working correctly**.

---

## 2. Random Array Generation ‚Äì `RandomArray.java`

This example demonstrates how to use the **[JCurand](https://javagl.de/jcuda.org/jcuda/jcurand/JCurand.html)** library to generate random numbers on the GPU and compare performance with a CPU implementation.

### What it does

* Generates an array of `n` random float values
* **CPU version**: Sequential random number generation
* **GPU version**: Random number generation using `JCurand`
* Measures and compares execution time for both approaches

### Key Concepts

* Using **CUDA libraries from Java**
* GPU-based random number generation
* Performance comparison between:

  * CPU (sequential)
  * GPU (parallel via JCuda)

This example shows that GPUs are not always the best choice: for small or trivial computations, the overhead of GPU execution can reduce the overall performance benefit.

---

## 3. Matrix Multiplication with Custom Kernel

Files involved:

* `MatrixMultiplication.java`
* `kernel.cu`

This is the **most advanced example** in the repository and demonstrates how to:

* Write a custom CUDA kernel
* Compile it to PTX
* Launch it from Java using JCuda
* Compare CPU vs GPU performance

### What it does

* Performs matrix multiplication: **C = A √ó B**
* Implements:

  * CPU version (sequential)
  * GPU version (CUDA kernel)
* Measures execution time under different conditions

### Performance Observations

* For large matrices (e.g. `n = m = k = 1024`):

  * **GPU is ~10√ó faster** when including memory allocation and transfers
  * **GPU is ~1000√ó faster** when comparing only:

    * Kernel execution + synchronization
    * CPU computation time

This clearly demonstrates that:

* GPU kernels are extremely fast
* Memory allocation and transfer overhead is significant
* Performance gains are highest for **large workloads**

---

## CUDA Kernel Compilation

The CUDA kernel is written in `kernel.cu` and must be compiled to **PTX** before running the Java code.

### Compile command

```bash
nvcc -ptx kernel.cu -o kernel.ptx
```

The generated `kernel.ptx` file is then loaded by `MatrixMultiplication.java` at runtime.

---

## Notes

* Performance depends heavily on GPU model and system configuration
* Results will vary for small input sizes due to overhead
* These examples are intended for **educational purposes**
